{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d24f4b26",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd325d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import BytesIO\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "from datetime import date\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "853c9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    parameters received from papermill\n",
    "\n",
    "    these are the parameters that are passed to the script\n",
    "    above is the default values\n",
    "    they can be overridden by the user when running the script\n",
    "\"\"\"\n",
    "\n",
    "BASE_URL = \"https://www.gov.br/anp/pt-br\"\n",
    "B100_SALES = f\"{BASE_URL}/assuntos/distribuicao-e-revenda/comercializacao-de-biodiesel\"\n",
    "RAW_PATH = \"data/raw/b100_sales\"\n",
    "\n",
    "bucket_name = os.getenv(\"GOOGLE_BUCKET_NAME\")\n",
    "start_date = \"2023-01-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "\n",
    "file_path = f\"gs://{bucket_name}/data/raw/b100_sales/b100_sales_{start_date}_{end_date}.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "267a7d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dwbessa/projects/vibra/pipeline_biomass_calculation/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get file download URL\n",
    "\"\"\"\n",
    "\n",
    "response = requests.get(B100_SALES, verify=False)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "if soup is None:\n",
    "    raise Exception(\"Failed to retrieve data from the URL.\")\n",
    "title_text = 'Volumes Comercializados de Biodiesel'\n",
    "title_h3 = soup.find('h3', string=title_text)\n",
    "jump_p = title_h3.find_next_sibling()\n",
    "jump_p = jump_p.find_next_sibling()\n",
    "year = jump_p.find_next_sibling()\n",
    "\n",
    "data_by_year = {}\n",
    "\n",
    "current_year = year.get_text(strip=True)\n",
    "data_by_year[current_year] = []\n",
    "\n",
    "next_elem = year.find_next_sibling()\n",
    "while next_elem:\n",
    "    if next_elem.name == 'ul':\n",
    "        for li in next_elem.find_all('li'):\n",
    "            a_tag = li.find('a')\n",
    "            if a_tag and 'href' in a_tag.attrs:\n",
    "                link = a_tag['href']\n",
    "                text = a_tag.get_text(strip=True)\n",
    "                li_text = li.get_text(strip=True)\n",
    "\n",
    "\n",
    "                start = li_text.find('Atualizado em ')\n",
    "                if start != -1:\n",
    "                    li_text = li_text[start + len('Atualizado em '):-1].strip()\n",
    "\n",
    "                data_by_year[current_year].append({'text': text, 'link': link, 'updated_date': li_text})\n",
    "            else:\n",
    "                print(\"Elemento <li> nÃ£o contÃ©m um link.\")\n",
    "        next_elem = next_elem.find_next_sibling()\n",
    "    else:\n",
    "        if next_elem.name == 'h3' and next_elem.get_text(strip=True).isdigit():\n",
    "            current_year = next_elem.get_text(strip=True)\n",
    "            if current_year not in data_by_year:\n",
    "                data_by_year[current_year] = []\n",
    "            next_elem = next_elem.find_next_sibling()\n",
    "            continue\n",
    "        else:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cfcaec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dwbessa/projects/vibra/pipeline_biomass_calculation/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "current_year = start_date.split(\"-\")[0] if start_date else date.today().year\n",
    "download_url =  data_by_year[current_year][0]['link']\n",
    "\n",
    "response = requests.get(download_url, verify=False)\n",
    "file_name = download_url.split(\"/\")[-1]\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"Falha ao baixar o arquivo: {response.status_code}\")\n",
    "\n",
    "file_bytes = response.content\n",
    "file_buffer = BytesIO(file_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22b68f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    - read the data from the file\n",
    "    - convert it to a pandas dataframe\n",
    "    - rename columns to lowercase and snake_case\n",
    "\n",
    "    B100_BRONZE_COLUMNS_MAPPING is a dictionary that maps the columns in the Excel file to the columns in the DataFrame\n",
    "\"\"\"\n",
    "B100_BRONZE_COLUMNS_MAPPING = {\n",
    "    \"2023\": {\n",
    "        \"Mês/Ano\": \"date\",\n",
    "        \"Raiz\\nCNPJ\": \"company_base_cnpj\",\n",
    "        \"Razão Social\": \"company_name\",\n",
    "        \"Quantidade\\nde Produto\\n(m³)\": \"volume_m3\"\n",
    "    },\n",
    "    \"2024\": {\n",
    "        \"Data\": \"date\",\n",
    "        \"Raiz de CNPJ do Distribuidor\": \"company_base_cnpj\",\n",
    "        \"Razão Social do Distribuidor\": \"company_name\",\n",
    "        \"Razão Social do Produtor\": \"producer_name\",\n",
    "        \"CNPJ do Produtor\": \"producer_cnpj\",\n",
    "        \"Volume (m3)\": \"volume_m3\"\n",
    "    }\n",
    "}\n",
    "\n",
    "map_to_use = B100_BRONZE_COLUMNS_MAPPING.get(current_year, B100_BRONZE_COLUMNS_MAPPING[\"2024\"])\n",
    "df = pd.read_excel(file_buffer, header=2, usecols=lambda x: 'Unnamed' not in x)\n",
    "df = df.rename(columns=map_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33bc03c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    change columns data types\n",
    "\"\"\"\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y%m')\n",
    "df[\"company_base_cnpj\"] = df[\"company_base_cnpj\"].astype(str).str.zfill(8)\n",
    "df['month'] = df['date'].dt.strftime('%Y-%m')\n",
    "df['date'] = df['date'].dt.date  # convert back to date if needed\n",
    "df['volume_m3'] = df['volume_m3'].astype(float)\n",
    "\n",
    "if int(current_year) < 2024:\n",
    "    df[\"producer_name\"] = None\n",
    "    df[\"producer_cnpj\"] = None\n",
    "else:\n",
    "    df[\"producer_cnpj\"] = df[\"producer_cnpj\"].astype(str).str.zfill(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a237009a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Months available in the dictionary: dict_keys(['2023-01', '2023-02', '2023-03', '2023-04', '2023-05', '2023-06', '2023-07', '2023-08', '2023-09', '2023-10', '2023-11', '2023-12'])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    get all datas to insert into BigQuery\n",
    "    remove empty dataframes\n",
    "    iterate over the dataframe and append the data to the corresponding month datafram\n",
    "    create a hashmap to store the dataframes for each month\n",
    "\"\"\"\n",
    "start_date_obj = pd.to_datetime(start_date).date()\n",
    "end_date_obj = pd.to_datetime(end_date).date()\n",
    "\n",
    "filter_by_start_and_end_date = (df['date'] >= start_date_obj) & (df['date'] <= end_date_obj)\n",
    "df = df[filter_by_start_and_end_date]\n",
    "df_by_month = {month: month_df.drop(columns='month') for month, month_df in df.groupby('month')}\n",
    "print(\"Months available in the dictionary:\", df_by_month.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28672d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data for partition: 202301\n"
     ]
    },
    {
     "ename": "BadRequest",
     "evalue": "400 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/named-embassy-456813-f3/jobs?uploadType=multipart: Partitioning specification must be provided in order to create partitioned table",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidResponse\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/vibra/pipeline_biomass_calculation/.venv/lib/python3.13/site-packages/google/cloud/bigquery/client.py:2629\u001b[39m, in \u001b[36mClient.load_table_from_file\u001b[39m\u001b[34m(self, file_obj, destination, rewind, size, num_retries, job_id, job_id_prefix, location, project, job_config, timeout)\u001b[39m\n\u001b[32m   2628\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_multipart_upload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_resource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m resumable_media.InvalidResponse \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/vibra/pipeline_biomass_calculation/.venv/lib/python3.13/site-packages/google/cloud/bigquery/client.py:3193\u001b[39m, in \u001b[36mClient._do_multipart_upload\u001b[39m\u001b[34m(self, stream, metadata, size, num_retries, timeout, project)\u001b[39m\n\u001b[32m   3189\u001b[39m     upload._retry_strategy = resumable_media.RetryStrategy(\n\u001b[32m   3190\u001b[39m         max_retries=num_retries\n\u001b[32m   3191\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3193\u001b[39m response = \u001b[43mupload\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransmit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_http\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_GENERIC_CONTENT_TYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   3195\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3197\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/vibra/pipeline_biomass_calculation/.venv/lib/python3.13/site-packages/google/resumable_media/requests/upload.py:153\u001b[39m, in \u001b[36mMultipartUpload.transmit\u001b[39m\u001b[34m(self, transport, data, metadata, content_type, timeout)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_request_helpers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_and_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretriable_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_status_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry_strategy\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/vibra/pipeline_biomass_calculation/.venv/lib/python3.13/site-packages/google/resumable_media/requests/_request_helpers.py:155\u001b[39m, in \u001b[36mwait_and_retry\u001b[39m\u001b[34m(func, get_status_code, retry_strategy)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     response = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _CONNECTION_ERROR_CLASSES \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/vibra/pipeline_biomass_calculation/.venv/lib/python3.13/site-packages/google/resumable_media/requests/upload.py:149\u001b[39m, in \u001b[36mMultipartUpload.transmit.<locals>.retriable_request\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    145\u001b[39m result = transport.request(\n\u001b[32m    146\u001b[39m     method, url, data=payload, headers=headers, timeout=timeout\n\u001b[32m    147\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/vibra/pipeline_biomass_calculation/.venv/lib/python3.13/site-packages/google/resumable_media/_upload.py:125\u001b[39m, in \u001b[36mUploadBase._process_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28mself\u001b[39m._finished = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[43m_helpers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequire_status_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_status_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/vibra/pipeline_biomass_calculation/.venv/lib/python3.13/site-packages/google/resumable_media/_helpers.py:108\u001b[39m, in \u001b[36mrequire_status_code\u001b[39m\u001b[34m(response, status_codes, get_status_code, callback)\u001b[39m\n\u001b[32m    107\u001b[39m         callback()\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m common.InvalidResponse(\n\u001b[32m    109\u001b[39m         response,\n\u001b[32m    110\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRequest failed with status code\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    111\u001b[39m         status_code,\n\u001b[32m    112\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected one of\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         *status_codes\n\u001b[32m    114\u001b[39m     )\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m status_code\n",
      "\u001b[31mInvalidResponse\u001b[39m: ('Request failed with status code', 400, 'Expected one of', <HTTPStatus.OK: 200>)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mBadRequest\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m partition_key = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmonth_num\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInserting data for partition: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpartition_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m job = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_table_from_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmonthly_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m$\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpartition_key\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_config\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m job.result()\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpartition_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m inserted successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/vibra/pipeline_biomass_calculation/.venv/lib/python3.13/site-packages/google/cloud/bigquery/client.py:2869\u001b[39m, in \u001b[36mClient.load_table_from_dataframe\u001b[39m\u001b[34m(self, dataframe, destination, num_retries, job_id, job_id_prefix, location, project, job_config, parquet_compression, timeout)\u001b[39m\n\u001b[32m   2867\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tmppath, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tmpfile:\n\u001b[32m   2868\u001b[39m         file_size = os.path.getsize(tmppath)\n\u001b[32m-> \u001b[39m\u001b[32m2869\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_table_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2870\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtmpfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2871\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2872\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2873\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrewind\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2874\u001b[39m \u001b[43m            \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mjob_id_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_id_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2877\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2879\u001b[39m \u001b[43m            \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_job_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2880\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2881\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2883\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   2884\u001b[39m     os.remove(tmppath)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/vibra/pipeline_biomass_calculation/.venv/lib/python3.13/site-packages/google/cloud/bigquery/client.py:2633\u001b[39m, in \u001b[36mClient.load_table_from_file\u001b[39m\u001b[34m(self, file_obj, destination, rewind, size, num_retries, job_id, job_id_prefix, location, project, job_config, timeout)\u001b[39m\n\u001b[32m   2629\u001b[39m         response = \u001b[38;5;28mself\u001b[39m._do_multipart_upload(\n\u001b[32m   2630\u001b[39m             file_obj, job_resource, size, num_retries, timeout, project=project\n\u001b[32m   2631\u001b[39m         )\n\u001b[32m   2632\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m resumable_media.InvalidResponse \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m-> \u001b[39m\u001b[32m2633\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_http_response(exc.response)\n\u001b[32m   2635\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m typing.cast(LoadJob, \u001b[38;5;28mself\u001b[39m.job_from_resource(response.json()))\n",
      "\u001b[31mBadRequest\u001b[39m: 400 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/named-embassy-456813-f3/jobs?uploadType=multipart: Partitioning specification must be provided in order to create partitioned table"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    insert data into BigQuery\n",
    "\"\"\"\n",
    "\n",
    "client = bigquery.Client()\n",
    "project_id = os.getenv(\"GOOGLE_PROJECT_ID\")\n",
    "bq_dataset = \"rw_ext_anp\"\n",
    "table_name = \"rw_ext_anp_b100_sales\"\n",
    "\n",
    "table_id = f\"{project_id}.{bq_dataset}.{table_name}\"\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    ")\n",
    "\n",
    "for month, monthly_df in df_by_month.items():\n",
    "    year = monthly_df['date'].iloc[0].year\n",
    "    month_num = monthly_df['date'].iloc[0].month\n",
    "    partition_key = f\"{year}{month_num:02d}\"\n",
    "\n",
    "    print(f\"Inserting data for partition: {partition_key}\")\n",
    "    job = client.load_table_from_dataframe(\n",
    "        monthly_df, f\"{table_id}${partition_key}\", job_config=job_config\n",
    "    )\n",
    "    job.result()\n",
    "    print(f\"Data for {partition_key} inserted successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomass-calc-automation-notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
