{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d24f4b26",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd325d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import BytesIO\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "from datetime import date, datetime\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "853c9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    parameters received from papermill\n",
    "\n",
    "    these are the parameters that are passed to the script\n",
    "    above is the default values\n",
    "    they can be overridden by the user when running the script\n",
    "\"\"\"\n",
    "\n",
    "BASE_URL = \"https://www.gov.br/anp/pt-br\"\n",
    "B100_SALES = f\"{BASE_URL}/assuntos/distribuicao-e-revenda/comercializacao-de-biodiesel\"\n",
    "RAW_PATH = \"data/raw/b100_sales\"\n",
    "\n",
    "bucket_name = os.getenv(\"GOOGLE_BUCKET_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "267a7d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dwbessa/projects/vibra/pipeline_biomass_calculation/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get file download URL\n",
    "\"\"\"\n",
    "\n",
    "response = requests.get(B100_SALES, verify=False)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "if soup is None:\n",
    "    raise Exception(\"Failed to retrieve data from the URL.\")\n",
    "title_text = 'Volumes Comercializados de Biodiesel'\n",
    "title_h3 = soup.find('h3', string=title_text)\n",
    "jump_p = title_h3.find_next_sibling()\n",
    "jump_p = jump_p.find_next_sibling()\n",
    "year = jump_p.find_next_sibling()\n",
    "\n",
    "data_by_year = {}\n",
    "\n",
    "current_year = year.get_text(strip=True)\n",
    "data_by_year[current_year] = []\n",
    "\n",
    "next_elem = year.find_next_sibling()\n",
    "while next_elem:\n",
    "    if next_elem.name == 'ul':\n",
    "        for li in next_elem.find_all('li'):\n",
    "            a_tag = li.find('a')\n",
    "            if a_tag and 'href' in a_tag.attrs:\n",
    "                link = a_tag['href']\n",
    "                text = a_tag.get_text(strip=True)\n",
    "                li_text = li.get_text(strip=True)\n",
    "\n",
    "\n",
    "                start = li_text.find('Atualizado em ')\n",
    "                if start != -1:\n",
    "                    li_text = li_text[start + len('Atualizado em '):-1].strip()\n",
    "\n",
    "                data_by_year[current_year].append({'text': text, 'link': link, 'updated_date': li_text})\n",
    "            else:\n",
    "                print(\"Elemento <li> nÃ£o contÃ©m um link.\")\n",
    "        next_elem = next_elem.find_next_sibling()\n",
    "    else:\n",
    "        if next_elem.name == 'h3' and next_elem.get_text(strip=True).isdigit():\n",
    "            current_year = next_elem.get_text(strip=True)\n",
    "            if current_year not in data_by_year:\n",
    "                data_by_year[current_year] = []\n",
    "            next_elem = next_elem.find_next_sibling()\n",
    "            continue\n",
    "        else:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6cfcaec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dwbessa/projects/vibra/pipeline_biomass_calculation/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "current_year = str(datetime.now().year)\n",
    "download_url =  data_by_year[current_year][0]['link']\n",
    "\n",
    "response = requests.get(download_url, verify=False)\n",
    "file_name = download_url.split(\"/\")[-1]\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"Falha ao baixar o arquivo: {response.status_code}\")\n",
    "\n",
    "file_bytes = response.content\n",
    "file_buffer = BytesIO(file_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "22b68f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    - read the data from the file\n",
    "    - convert it to a pandas dataframe\n",
    "    - rename columns to lowercase and snake_case\n",
    "\n",
    "    B100_BRONZE_COLUMNS_MAPPING is a dictionary that maps the columns in the Excel file to the columns in the DataFrame\n",
    "\"\"\"\n",
    "B100_BRONZE_COLUMNS_MAPPING = {\n",
    "    \"2023\": {\n",
    "        \"Mês/Ano\": \"vb100_dat_compra\",\n",
    "        \"Raiz\\nCNPJ\": \"vb100_txt_cnpj\",\n",
    "        \"Razão Social\": \"vb100_txt_razao_social\",\n",
    "        \"Quantidade\\nde Produto\\n(m³)\": \"vb100_qtd_volume\"\n",
    "    },\n",
    "    \"2024\": {\n",
    "        \"Data\": \"vb100_dat_compra\",\n",
    "        \"Raiz de CNPJ do Distribuidor\": \"vb100_txt_cnpj\",\n",
    "        \"Razão Social do Distribuidor\": \"vb100_txt_razao_social\",\n",
    "        \"Razão Social do Produtor\": \"vb100_txt_produtor\",\n",
    "        \"CNPJ do Produtor\": \"vb100_txt_produtor_cnpj\",\n",
    "        \"Volume (m3)\": \"vb100_qtd_volume\"\n",
    "    }\n",
    "}\n",
    "\n",
    "map_to_use = B100_BRONZE_COLUMNS_MAPPING.get(current_year, B100_BRONZE_COLUMNS_MAPPING[\"2024\"])\n",
    "df = pd.read_excel(file_buffer, header=2, usecols=lambda x: 'Unnamed' not in x)\n",
    "df = df.rename(columns=map_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bc03c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[us, America/Sao_Paulo]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    change columns data types\n",
    "\"\"\"\n",
    "\n",
    "df['vb100_dat_compra'] = pd.to_datetime(df['vb100_dat_compra'], format='%Y%m')\n",
    "df[\"vb100_txt_cnpj\"] = df[\"vb100_txt_cnpj\"].astype(str).str.zfill(8)\n",
    "df['vb100_dat_compra'] = df['vb100_dat_compra'].dt.date\n",
    "df['vb100_qtd_volume'] = df['vb100_qtd_volume'].astype(float)\n",
    "df['vb100_dat_criacao'] = pd.Timestamp.now(tz='America/Sao_Paulo')\n",
    "\n",
    "if int(current_year) < 2024:\n",
    "    df[\"vb100_txt_produtor\"] = None\n",
    "    df[\"vb100_txt_produtor_cnpj\"] = None\n",
    "else:\n",
    "    df[\"vb100_txt_produtor_cnpj\"] = df[\"vb100_txt_produtor_cnpj\"].astype(str).str.zfill(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28672d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data for partition: 20250703\n",
      "  Data for 20250703 inserted successfully.\n",
      "Data insertion completed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    insert data into BigQuery with date-based partitioning\n",
    "\"\"\"\n",
    "\n",
    "client = bigquery.Client()\n",
    "project_id = os.getenv(\"GOOGLE_CLOUD_PROJECT\")\n",
    "bq_dataset = \"rw_ext_anp\"\n",
    "table_name = \"venda_b100\"\n",
    "\n",
    "table_id = f\"{project_id}.{bq_dataset}.{table_name}\"\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    ")\n",
    "\n",
    "\n",
    "partition_key = date.today().strftime('%Y%m%d')\n",
    "\n",
    "partitioned_table_id = f\"{table_id}${partition_key}\"\n",
    "print(f\"Inserting data for partition: {partition_key}\")\n",
    "\n",
    "try:\n",
    "    job = client.load_table_from_dataframe(\n",
    "        df, partitioned_table_id, job_config=job_config\n",
    "    )\n",
    "    job.result()\n",
    "    print(f\"  Data for {partition_key} inserted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error inserting data for {partition_key}: {str(e)}\")\n",
    "\n",
    "print(\"Data insertion completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomass-calc-automation-notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
